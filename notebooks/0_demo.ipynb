{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter Kernel**:\n",
    "\n",
    "* If you are in SageMaker Notebook instance, please make sure you are using **conda_pytorch_latest_p36** kernel\n",
    "* If you are on SageMaker Studio, please make sure you are using **SageMaker JumpStart PyTorch 1.0** kernel\n",
    "\n",
    "**Run All**:\n",
    "\n",
    "* If you are in SageMaker notebook instance, you can go to *Cell tab -> Run All*\n",
    "* If you are in SageMaker Studio, you can go to *Run tab -> Run All Cells*\n",
    "\n",
    "**Note**: To *Run All* successfully, make sure you have executed the entire demo notebook `0_demo.ipynb` first.\n",
    "\n",
    "## SageMaker Defect Detection Demo\n",
    "\n",
    "In this notebook, we deploy an endpoint from a provided pretrained detection model that was already trained on **NEU-DET** dataset. Then, we send some image samples with defects for detection and visual the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_config = json.load(open(\"../stack_outputs.json\"))\n",
    "role = sagemaker_config[\"IamRole\"]\n",
    "solution_bucket = sagemaker_config[\"SolutionS3Bucket\"]\n",
    "region = sagemaker_config[\"AWSRegion\"]\n",
    "solution_name = sagemaker_config[\"SolutionName\"]\n",
    "bucket = sagemaker_config[\"S3Bucket\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download our **NEU-DET** dataset from our public S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_bucket = f\"s3://{solution_bucket}-{region}/{solution_name}\"\n",
    "original_data_prefix = \"data/NEU-DET.zip\"\n",
    "original_data = f\"{original_bucket}/{original_data_prefix}\"\n",
    "original_pretained_checkpoint = f\"{original_bucket}/pretrained\"\n",
    "original_sources = f\"{original_bucket}/build/lib/source_dir.tar.gz\"\n",
    "print(\"original data: \")\n",
    "S3Downloader.list(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easiler data processing, depending on the dataset, we unify the class and label names using the scripts from `prepare_data` which should take less than **5 minutes** to complete. This is done once throughout all our notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "RAW_DATA_PATH = !echo $PWD/raw_neu_det\n",
    "RAW_DATA_PATH = RAW_DATA_PATH.n\n",
    "DATA_PATH = !echo $PWD/neu_det\n",
    "DATA_PATH = DATA_PATH.n\n",
    "\n",
    "!mkdir -p $RAW_DATA_PATH\n",
    "!aws s3 cp $original_data $RAW_DATA_PATH\n",
    "\n",
    "!mkdir -p $DATA_PATH\n",
    "!python ../src/prepare_data/neu.py $RAW_DATA_PATH/NEU-DET.zip $DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data preparation, we need upload the prepare data to S3 and setup some paths that will be used throughtout the notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prefix = \"neu-det\"\n",
    "neu_det_s3 = f\"s3://{bucket}/{prefix}\"\n",
    "sources = f\"{neu_det_s3}/code/\"\n",
    "train_output = f\"{neu_det_s3}/output/\"\n",
    "neu_det_prepared_s3 = f\"{neu_det_s3}/data/\"\n",
    "!aws s3 sync $DATA_PATH $neu_det_prepared_s3 --quiet # remove the --quiet flag to view the sync logs\n",
    "s3_checkpoint = f\"{neu_det_s3}/checkpoint/\"\n",
    "sm_local_checkpoint_dir = \"/opt/ml/checkpoints/\"\n",
    "s3_pretrained = f\"{neu_det_s3}/pretrained/\"\n",
    "!aws s3 sync $original_pretained_checkpoint $s3_pretrained\n",
    "!aws s3 ls $s3_pretrained\n",
    "!aws s3 cp $original_sources $sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let examine some datasets that we will use later by providing an `ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "try:\n",
    "    import sagemaker_defect_detection\n",
    "except ImportError:\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    ROOT = Path(\"../src\").resolve()\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from sagemaker_defect_detection import NEUDET, get_preprocess\n",
    "\n",
    "SPLIT = \"test\"\n",
    "ID = 10\n",
    "assert 0 <= ID <= 300\n",
    "dataset = NEUDET(DATA_PATH, split=SPLIT, preprocess=get_preprocess())\n",
    "images, targets, _ = dataset[ID]\n",
    "original_image = copy.deepcopy(images)\n",
    "original_boxes = targets[\"boxes\"].numpy().copy()\n",
    "original_labels = targets[\"labels\"].numpy().copy()\n",
    "print(f\"first images size: {original_image.shape}\")\n",
    "print(f\"target bounding boxes: \\n {original_boxes}\")\n",
    "print(f\"target labels: {original_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now visualize it using the provided utilities as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_defect_detection.utils.visualize import unnormalize_to_hwc, visualize\n",
    "\n",
    "original_image_unnorm = unnormalize_to_hwc(original_image)\n",
    "\n",
    "visualize(\n",
    "    original_image_unnorm,\n",
    "    [original_boxes],\n",
    "    [original_labels],\n",
    "    colors=[(255, 0, 0)],\n",
    "    titles=[\"original\", \"ground truth\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our demo, we deploy an endpoint using a provided pretrained checkpoint. It takes about **10 minutes** to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from os import path as osp\n",
    "\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "demo_model = PyTorchModel(\n",
    "    osp.join(s3_pretrained, \"model.tar.gz\"),\n",
    "    role,\n",
    "    entry_point=\"detector.py\",\n",
    "    source_dir=osp.join(sources, \"source_dir.tar.gz\"),\n",
    "    framework_version=\"1.5\",\n",
    "    py_version=\"py3\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    name=sagemaker_config[\"SolutionPrefix\"] + \"-demo-model\",\n",
    ")\n",
    "\n",
    "demo_detector = demo_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    endpoint_name=sagemaker_config[\"SolutionPrefix\"] + \"-demo-endpoint\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We change the input depending on whether we are providing a list of images or a single image. Also the model requires a four dimensional array / tensor (with the first dimension as batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = list(img.numpy() for img in images) if isinstance(images, list) else images.unsqueeze(0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the input is ready and we can get some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# SageMaker 1.x doesn't allow_pickle=True by default\n",
    "from pkg_resources import parse_version\n",
    "np_load_old = np.load\n",
    "if parse_version(sagemaker.__version__) < parse_version('2'):\n",
    "    np.load = lambda *args, **kwargs: np_load_old(*args, allow_pickle=True, **kwargs)  \n",
    "demo_predictions = demo_detector.predict(input)\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we visualize them as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize(\n",
    "    original_image_unnorm,\n",
    "    [original_boxes, demo_predictions[0][\"boxes\"]],\n",
    "    [original_labels, demo_predictions[0][\"labels\"]],\n",
    "    colors=[(255, 0, 0), (0, 0, 255)],\n",
    "    titles=[\"original\", \"ground truth\", \"pretrained demo\"],\n",
    "    dpi=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Delete the endpoint and model\n",
    "\n",
    "**Note:** to follow all the notebooks, it is required to keep demo model and the demo endpoint. It will be automatically deleted when you delete the entire resources/stack. However, if you need to, please uncomment and run the next cell\n",
    "\n",
    "All of the training jobs, models and endpoints we created can be viewed through the SageMaker console of your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_detector.delete_model()\n",
    "# demo_detector.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Click here to continue](./1_retrain_from_checkpoint.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pytorch_latest_p36': conda)",
   "name": "python361064bitpytorchlatestp36conda2dfac45b320c45f3a4d1e89ca46b60d1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
